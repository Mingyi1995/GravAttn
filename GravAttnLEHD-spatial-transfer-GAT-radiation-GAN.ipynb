{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9687af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingyi/anaconda3/envs/gnn/lib/python3.10/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "import networkx as nx\n",
    "from netAPI import adjacency_matrix\n",
    "\n",
    "from GNNAPI import buildVNNConfig\n",
    "# from GNNAPI import EarlyStopper\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "#from NetRepLearnerV2 import NetGNNAttRepr\n",
    "from NetRepLearner_mh import NetReprLearning\n",
    "# from NetRepLearner_mh import GMLearning\n",
    "import pandas as pd\n",
    "from netAPI import pickleLoad, pickleDump\n",
    "from GNNAPI import VNN_MLP\n",
    "from GNNAPI import GNN_VNN_Layer\n",
    "from GNNAPI import buildLaplacian\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist,pdist\n",
    "import scipy.stats\n",
    "# from GNNAPI import DirectTensor\n",
    "import scipy.optimize\n",
    "import scipy.stats as stats\n",
    "\n",
    "from netAPI import loadNetworkMat\n",
    "from netAPI import adjacency_matrix\n",
    "from GNNAPI import GNN_VNN_Layer\n",
    "from GNNAPI import VNNDefaultConfig\n",
    "from GNNAPI import buildLaplacian\n",
    "from GNNAPI import buildVNNConfig\n",
    "# from GNNAPI import matrixNormalize\n",
    "# from GNNAPI import GNN_VNN_Multiclass_Layer\n",
    "from GNNAPI import NNmodel\n",
    "from GNNAPI import dmerge\n",
    "import time\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from esda.moran import Moran\n",
    "from esda.geary import Geary\n",
    "import libpysal\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from visdom import Visdom\n",
    "import warnings\n",
    "from csv import writer\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc16725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inbetween(o,d,A,win,wout):\n",
    "    distance = A[o,d]\n",
    "    \n",
    "    if win[A[o,:]<distance].shape[0]>0: \n",
    "        # number of available jobs in a shorter distance\n",
    "        ofts = win[A[o,:]<distance].sum(axis=0,keepdims=True)[0]\n",
    "    else:\n",
    "        ofts = 0\n",
    "    if wout[A[d,:]<distance].shape[0]>0: \n",
    "        # number of available residences in a shorter distance\n",
    "        dfts = wout[A[d,:]<distance].sum(axis=0,keepdims=True)[0]\n",
    "    else:\n",
    "        dfts = 0\n",
    "#     print(ofts,dfts,win[d],wout[o])\n",
    "    ofts = ofts/(win[d]+1)\n",
    "    dfts = dfts/(wout[o]+1)\n",
    "    between_fts = torch.FloatTensor([ofts,dfts,distance]).view(1,3)\n",
    "#     print(between_fts)\n",
    "    return between_fts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59777ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopper2:\n",
    "    def __init__(self, patience=1):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_gap = 1e9\n",
    "        self.updated_time = 0\n",
    "\n",
    "    def early_stop(self, loss,ad_loss):\n",
    "        if np.abs(loss-ad_loss) < self.min_gap:\n",
    "            self.min_gap = np.abs(loss-ad_loss)\n",
    "            self.counter = 0\n",
    "            self.updated_time += 1\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience and self.updated_time>1:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d7eb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialTrainer:\n",
    "    def __init__(self, input_dim, feature_dim, learning_rate=1e-3):\n",
    "        torch.manual_seed(0)\n",
    "        # Initialize models\n",
    "        self.feature_extractor = self.FeatureExtractor(input_dim, feature_dim)\n",
    "        self.domain_discriminator = self.DomainDiscriminator(input_dim,feature_dim)\n",
    "\n",
    "        # Loss and Optimizers\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer_F = optim.Adam(self.feature_extractor.parameters(), lr=learning_rate)\n",
    "        self.optimizer_D = optim.Adam(self.domain_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    class FeatureExtractor(nn.Module):\n",
    "        def __init__(self, input_dim, feature_dim):\n",
    "            super().__init__()\n",
    "            self.model1 = nn.Sequential(\n",
    "                nn.Linear(input_dim, feature_dim),\n",
    "                nn.ReLU(),\n",
    "\n",
    "            )\n",
    "            self.model2 = nn.Sequential(\n",
    "                nn.Linear(feature_dim, feature_dim),\n",
    "                nn.ReLU())\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            \n",
    "            e = self.model1(x)\n",
    "            e = self.model2(e)+e\n",
    "#             e = self.model3(e)\n",
    "            return e\n",
    "\n",
    "    class DomainDiscriminator(nn.Module):\n",
    "        def __init__(self,input_dim, feature_dim):\n",
    "            super().__init__()\n",
    "            self.model1 = nn.Sequential(\n",
    "                nn.Linear(feature_dim, feature_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(feature_dim, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.model2 = nn.Sequential(\n",
    "                nn.Linear(feature_dim, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.model1(x) + x\n",
    "            x = self.model2(x)\n",
    "            return x\n",
    "    def cuda(self):\n",
    "        self.feature_extractor = self.feature_extractor.cuda()\n",
    "        self.domain_discriminator = self.domain_discriminator.cuda()\n",
    "\n",
    "    def train(self, source_data, target_data, source_labels, target_labels,valid_data,valid_labels, num_epochs=2000):\n",
    "        early_stopper = EarlyStopper2(patience=5)\n",
    "#         viz = Visdom()\n",
    "#         viz.line([0.], [0.], win='fts extractor',name='train')\n",
    "#         viz.line([0.], [0.], win='fts extractor',name='extractor')\n",
    "#         viz.line([0.], [0.], win='fts extractor',name='discriminator')\n",
    "        for epoch in range(num_epochs):\n",
    "            # Train feature extractor and domain discriminator\n",
    "            self.optimizer_F.zero_grad()\n",
    "            self.optimizer_D.zero_grad()\n",
    "\n",
    "            # Source domain\n",
    "            source_features = self.feature_extractor(source_data)\n",
    "            source_domain_preds = self.domain_discriminator(source_features)\n",
    "            loss_source = self.criterion(source_domain_preds, (1-source_labels))\n",
    "\n",
    "            # Target domain\n",
    "            target_features = self.feature_extractor(target_data)\n",
    "            target_domain_preds = self.domain_discriminator(target_features)\n",
    "            loss_target = self.criterion(target_domain_preds, (1-target_labels))\n",
    "\n",
    "            loss = loss_source + loss_target\n",
    "            loss.backward()\n",
    "#             viz.line([loss.item()], [epoch], win='fts extractor',name='extractor', update='append')\n",
    "\n",
    "            self.optimizer_F.step()\n",
    "#             viz.line([loss.item()+0.6931], [epoch], win='fts extractor',name='extractor', update='append')\n",
    "            \n",
    "            self.optimizer_D.zero_grad()\n",
    "\n",
    "            source_features = self.feature_extractor(source_data)\n",
    "            source_domain_preds = self.domain_discriminator(source_features)\n",
    "            loss_source_adversarial = self.criterion(source_domain_preds, source_labels)\n",
    "\n",
    "            target_features = self.feature_extractor(target_data)\n",
    "            target_domain_preds = self.domain_discriminator(target_features)\n",
    "            loss_target_adversarial = self.criterion(target_domain_preds, target_labels)\n",
    "\n",
    "            loss_adversarial = loss_source_adversarial + loss_target_adversarial\n",
    "       \n",
    "            loss_adversarial.backward()\n",
    "\n",
    "            self.optimizer_D.step()\n",
    "#             viz.line([loss_adversarial.item()], [epoch], win='fts extractor',name='discriminator', update='append')\n",
    "\n",
    "#             viz.line([validation_loss], [epoch], win='fts extractor',name='validation_loss', update='append')\n",
    "\n",
    "            if early_stopper.early_stop(loss.item(),loss_adversarial.item()):\n",
    "                print('feature extractor early stop at '+str(epoch)+' epochs')             \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2cb7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMLearning(NetReprLearning): #mobility model learning class\n",
    "\n",
    "    def initNNs(self,fts, GNNConfigs, VNNConfig):\n",
    "        torch.manual_seed(0)\n",
    "        self.embed_dim = GNNConfigs['out_features']\n",
    "        # mh edited, previous infeautre for GNN layer was N nodes as X is none\n",
    "        \n",
    "        # mh: v['out_features'] for v in GNNConfigs: self.N, ed\n",
    "        featuredims = [fts.shape[1]] + [GNNConfigs['out_features']]\n",
    "        print('featuredims',featuredims)\n",
    "        self.embed_dim = featuredims[-1]\n",
    "        self.GNNLayerNum = len(GNNConfigs)\n",
    "#         self.GNNLayers = [GNN_VNN_Layer(in_features = featuredims[i], \n",
    "#                         VNNConfig = GNNConfigs[i]) for i in range(self.GNNLayerNum)]\n",
    "        self.num_heads = GNNConfigs['transformer_num_heads']\n",
    "        self.gat_num_heads = GNNConfigs['gat_num_heads']\n",
    "        self.input_dim = featuredims[0]\n",
    "        self.attention_out_dim = GNNConfigs['attention_out_dim']\n",
    "        self.edge_dim = GNNConfigs['edge_dim']\n",
    "        \n",
    "\n",
    "        self.shortcut1 = nn.Linear(self.input_dim, GNNConfigs['layer_dims'][0]* self.gat_num_heads)\n",
    "        self.shortcut2 = nn.Linear(GNNConfigs['layer_dims'][0]* self.gat_num_heads,\n",
    "                                   GNNConfigs['out_features'])\n",
    "        \n",
    "        self.in_conv = GATConv(self.input_dim, GNNConfigs['layer_dims'][0],edge_dim=self.edge_dim,\n",
    "                               heads=self.gat_num_heads,concat=True)\n",
    "#         Hidden layers\n",
    "\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "        for i,hidden in enumerate(GNNConfigs['layer_dims']):\n",
    "            if i + 1 < len(GNNConfigs['layer_dims']):\n",
    "                self.hidden_layers.append(\n",
    "                    GATConv(hidden * self.gat_num_heads, GNNConfigs['layer_dims'][i+1], \n",
    "                            heads=self.gat_num_heads,\n",
    "                             concat=True)\n",
    "                )\n",
    "        \n",
    "        # Output layer\n",
    "        self.out_conv = GATConv(GNNConfigs['layer_dims'][-1] * self.gat_num_heads, \n",
    "                                GNNConfigs['out_features'], heads=self.gat_num_heads, \n",
    "                                concat=False)\n",
    "\n",
    "        # Hidden layers\n",
    "       \n",
    "       \n",
    "        self.OutMLP = nn.Linear(GNNConfigs['out_features']*2,1)\n",
    "\n",
    "        # Linear layers for query, key, and value projections\n",
    "        self.query_projection = nn.Parameter(torch.randn(self.embed_dim,self.attention_out_dim,device='cuda'))                                                                                             \n",
    "        self.key_projection = nn.Parameter(torch.randn(self.embed_dim,self.attention_out_dim,device='cuda'))\n",
    "        self.expo = nn.Parameter(torch.randn(1,1,device='cuda'))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,A, X,OD,between_fts): #feed-forward computation - embedding and the attractivity scores\n",
    "        wout = OD.sum(axis = 1,keepdims=True)\n",
    "        win = OD.sum(axis = 0,keepdims=True)\n",
    "#         OD[OD==0] = 0.1\n",
    "#         win[win==0] = 0.2\n",
    "        W = wout * win / win.sum()\n",
    "        #         print(type(wout),type(win),type(W))\n",
    "\n",
    "        wout = wout.to(torch.device(\"cuda\"))\n",
    "        self.win = win.to(torch.device(\"cuda\"))\n",
    "        W = W.to(torch.device(\"cuda\"))\n",
    "#         X = X.to(torch.device(\"cuda\"))\n",
    "        if self.model == 'p':\n",
    "            rows = []\n",
    "            cols = []\n",
    "            for i in range(OD.shape[0]):\n",
    "                for j in range(OD.shape[0]):\n",
    "                    if i != j:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "            edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
    "            edge_index = edge_index.to(torch.device(\"cuda\"))\n",
    "#             print(X.device,edge_index.device,between_fts.device)\n",
    "            E = self.in_conv(X, edge_index,edge_attr=between_fts)\n",
    "\n",
    "            E += self.shortcut1(X)\n",
    "            E = nn.Sigmoid()(E)\n",
    "#             print('----------ReLU-----------------')\n",
    "#             print(E)\n",
    "\n",
    "\n",
    "            for layer in self.hidden_layers:\n",
    "                E = layer(E, edge_index)+E\n",
    "                E = nn.Sigmoid()(E)\n",
    "\n",
    "            E = self.out_conv(E, edge_index)+self.shortcut2(E)\n",
    "\n",
    "            E = nn.Sigmoid()(E)\n",
    "        if self.attention:\n",
    "            seq_len, input_dim = E.size()\n",
    "\n",
    "            query = torch.matmul(E,self.query_projection)\n",
    "#             print('==============query==================')\n",
    "#             print(query)\n",
    "            key =torch.matmul(E,self.key_projection)\n",
    "#             print('==============key===================')\n",
    "#             print(key)\n",
    "            # Reshape the query, key, and value tensors to enable multi-head attention\n",
    "            query = query.view(seq_len, self.num_heads,self.attention_out_dim // self.num_heads).permute(1, 0, 2)\n",
    "            key = key.view(seq_len, self.num_heads, self.attention_out_dim  // self.num_heads).permute(1, 0, 2)\n",
    "\n",
    "            # Compute the dot product attention scores\n",
    "            scores = torch.matmul(query, key.permute(0, 2, 1))\n",
    "\n",
    "            scores = scores / torch.sqrt(torch.tensor(input_dim*2 / self.num_heads, dtype=torch.float32))\n",
    "            attention_weights = torch.mean(scores,axis=0)\n",
    "            attention_weights = nn.ReLU()(attention_weights)\n",
    "            y = attention_weights\n",
    "        elif self.VNNattraction:\n",
    "            edge_index = list(zip(sorted(list(range(E.shape[0]))*E.shape[0]),\n",
    "                                      list(range(E.shape[0]))*E.shape[0]))\n",
    "            edge_index = np.array(edge_index)\n",
    "\n",
    "            PE_ = torch.concat([E[edge_index[:,0]],E[edge_index[:,1]]],axis=1)\n",
    "#             print(E)\n",
    "\n",
    "            batch_size = 10000\n",
    "            num_batches = PE_.shape[0] // batch_size\n",
    "            for i in range(num_batches+1):\n",
    "                # Generate mini-batch indices\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = (i + 1) * batch_size\n",
    "                if end_idx <= PE_.shape[0]:\n",
    "                    pass\n",
    "                else:\n",
    "                    end_idx = PE_.shape[0]+1\n",
    "\n",
    "                # Extract mini-batch data and labels\n",
    "                PE_batch = PE_[start_idx:end_idx]\n",
    "                if i == 0:\n",
    "                    y = self.OutMLP.forward(PE_batch)\n",
    "                    y = nn.Sigmoid()(y)\n",
    "                    y = F.dropout(y,p=0.2)\n",
    "                else:\n",
    "                    y_temp = self.OutMLP.forward(PE_batch)\n",
    "                    y_temp = nn.Sigmoid()(y_temp)\n",
    "                    y_temp = F.dropout(y_temp,p=0.2)\n",
    "                    y = torch.concat([y,y_temp])\n",
    "            y = y.view(E.shape[0], E.shape[0])\n",
    "            \n",
    "        if self.model == 'p':\n",
    "#             A_tensor = torch.FloatTensor(A)\n",
    "#             A_tensor = A.to(torch.device(\"cuda\"))\n",
    "#             print(y)\n",
    "            y = wout * (self.win  * torch.exp(-y*A)) /\\\n",
    "                        (self.win  * torch.exp(-y*A)).sum(axis=1,keepdims=True)\n",
    "  \n",
    "\n",
    "\n",
    "            \n",
    "        elif self.model == 'g':\n",
    "            y = wout * (self.win * torch.exp(self.expo*A) /\\\n",
    "                        (self.win  * torch.exp(self.expo*A)).sum(axis=1,keepdims=True))\n",
    "\n",
    "\n",
    "    \n",
    "        return y\n",
    "\n",
    "    def logMSE(self, Y, Ytrue): #log MSE loss for the unconstrained model\n",
    "#         print(Y.device,Ytrue.device,self.W.device,mask.device)\n",
    "        \n",
    "        loss_ =  ((torch.log(Ytrue + 1) -\\\n",
    "                                   torch.log(Y + 1)) ** 2).mean()\n",
    "#         loss_ = (torch.mul(mask , (Ytrue -Y ) ** 2)).sum() / mask.sum()\n",
    "        return loss_\n",
    "    def MSE(self, Y, Ytrue): #log MSE loss for the unconstrained model\n",
    "#         print(Y.device,Ytrue.device,self.W.device,mask.device)\n",
    "        loss_ =  ((Ytrue - Y ) ** 2).mean()\n",
    "#         loss_ = (torch.mul(mask , (Ytrue -Y ) ** 2)).sum() / mask.sum()\n",
    "        return loss_\n",
    "    def LL(self, p, Ytrue): #log MSE loss for the unconstrained model\n",
    "        loss_ =  -(Ytrue*torch.log(p)).mean()\n",
    "        return loss_\n",
    "\n",
    "    def KL(self, Y, Ytrue): #log MSE loss for the unconstrained model\n",
    "\n",
    "        loss_ =  (Ytrue*torch.log((Ytrue+1e-8)/(Y+1e-8))).mean()\n",
    "\n",
    "#         print(loss_)\n",
    "        return loss_\n",
    "\n",
    "    def RAE(self, Y, Ytrue): #log MSE loss for the unconstrained model\n",
    "#         print(Y.device,Ytrue.device,self.W.device,mask.device)\n",
    "        ybar = Ytrue.mean()\n",
    "        loss_ =  (torch.absolute(Ytrue - Y )).sum()/(torch.absolute(Ytrue - ybar )).sum()\n",
    "#         loss_ = (torch.mul(mask , (Ytrue -Y ) ** 2)).sum() / mask.sum()\n",
    "        return loss_\n",
    "    def chi(self, Y, Ytrue): #log MSE loss for the unconstrained model\n",
    "\n",
    "        loss_ =  (((Ytrue - Y )**2)/(Y+1e-8)).mean()\n",
    "        return loss_\n",
    "\n",
    "    def loss(self, Y = None, Ytrue = None): #compute loss\n",
    "\n",
    "        return self.KL(Y,Ytrue)\n",
    "\n",
    "    def initEmbed(self, x): #initialize node embedding with some initial coordinates\n",
    "        self.GNNLayers[0].init_params({'X': x})\n",
    "\n",
    "# mh edited for mask by nodes\n",
    "def getTrainTestbyNodes(A, train_p = 0.7, seed = 1): #train-test split of the network edges\n",
    "    np.random.seed(seed)\n",
    "    number = np.random.uniform(size = A.shape[0])\n",
    "\n",
    "    valid = number < (1-train_p)**2\n",
    "    train = np.array([True if i < train_p and i >= (1-train_p)**2 else False for i in number ])\n",
    "    test = number > train_p\n",
    "    \n",
    "\n",
    "    return train,valid, test\n",
    "\n",
    "def read_area_from_ct(state,nodes):\n",
    "    ct_map = gpd.read_file('LEHD/'+state+'.zip')\n",
    "    ct_map['GEOID'] = ct_map['GEOID'].astype(int)\n",
    "    ct_map = ct_map.set_index('GEOID')\n",
    "    ct_map = ct_map.loc[nodes]\n",
    "\n",
    "    return ct_map.to_crs('ESRI:102008').geometry.area.values\n",
    "\n",
    "def read_location_from_ct(state,nodes):\n",
    "    ct_map = gpd.read_file('LEHD/'+state+'.zip')\n",
    "    ct_map['GEOID'] = ct_map['GEOID'].astype(int)\n",
    "    ct_map = ct_map.set_index('GEOID')\n",
    "    ct_map = ct_map.loc[sorted(list(nodes))]\n",
    "    ct_map = ct_map.to_crs('epsg:4326')\n",
    "    return np.array(list(zip(ct_map.geometry.centroid.x,\n",
    "                    ct_map.geometry.centroid.y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f535376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(v, v_hat, axis=None):\n",
    "        '''\n",
    "        Mean absolute percentage error.\n",
    "        :param v: np.ndarray or int, ground truth.\n",
    "        :param v_: np.ndarray or int, prediction.\n",
    "        :param axis: axis to do calculation.\n",
    "        :return: int, MAPE averages on all elements of input.\n",
    "        '''\n",
    "#         mask_0 = (v == 0)\n",
    "#         percentage = (torch.abs(v_hat - v)) / (torch.abs(v)+1)\n",
    "        error = torch.abs(v_hat - v)\n",
    "        v[v==0]=1\n",
    "        mape = ( error/ torch.abs(v)).mean()\n",
    "#         if torch.any(mask):\n",
    "#             masked_array = torch.mean(torch.mul(mask,percentage))  # mask the dividing-zero as invalid\n",
    "#             result = masked_array.mean(axis=axis)\n",
    "            \n",
    "        return mape\n",
    "\n",
    "def common_part_of_commuters(values1, values2, numerator_only=False):\n",
    "    if type(values1)  == torch.Tensor:\n",
    "        pass\n",
    "    else:\n",
    "        values1 = torch.FloatTensor(values1)\n",
    "        values2 = torch.FloatTensor(values2)\n",
    "    if numerator_only:\n",
    "        tot = 1.0\n",
    "    else:\n",
    "        tot = (torch.sum(values1) + torch.sum(values2))\n",
    "#         print( np.sum(values1))\n",
    "    if tot > 0:\n",
    "        return 2.0 * torch.sum(torch.minimum(values1, values2)) / tot\n",
    "    else:\n",
    "        return 0.0\n",
    "def MAE(Y, Ytrue): #log MSE loss for the unconstrained model\n",
    "\n",
    "#         print(Y.device,Ytrue.device,self.W.device,mask.device)\n",
    "    loss_ = torch.abs(Ytrue -Y).mean()\n",
    "    return loss_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78bbcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(current, total, bar_length=20):\n",
    "    fraction = current / total\n",
    "\n",
    "    arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "    padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "    ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "    print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e7285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47715ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading datasets\n",
      "loading target LEHD: New York City\n",
      "featuredims [46, 16]\n",
      "early stop at 115 epochs\n",
      "KL 0.464\n",
      "CPC 0.594\n",
      "MAE 0.535\n",
      "pearson 0.767\n",
      "jenson 0.41\n",
      "loading target LEHD: Los Angeles\n",
      "featuredims [46, 16]\n",
      "early stop at 79 epochs\n",
      "KL 0.521\n",
      "CPC 0.52\n",
      "MAE 0.493\n",
      "pearson 0.666\n",
      "jenson 0.466\n",
      "loading target LEHD: Chicago\n",
      "featuredims [46, 16]\n",
      "early stop at 90 epochs\n",
      "KL 0.751\n",
      "CPC 0.59\n",
      "MAE 0.866\n",
      "pearson 0.769\n",
      "jenson 0.402\n",
      "loading target LEHD: Houston\n",
      "featuredims [46, 16]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splitByNodes = True\n",
    "externalities = True\n",
    "# if modelmode='g', conventional singly constrained gravity model, other parameters will be ignored\n",
    "modelmode = 'p'\n",
    "# gravi con, modelmode = 'p', attention = True, directEmebdding = False\n",
    "# GCN + MLP, modelmode = 'p', attention = False, VNNattraction = Truepath = 'LEHD/'\n",
    "seed = 0\n",
    "cities = [\n",
    "    ('New York City', 'ny', ['New York County, NY', 'Queens County, NY','Kings County, NY','Bronx County, NY','Richmond County, NY']),\n",
    "    ('Los Angeles', 'ca', ['Los Angeles County, CA']),\n",
    "    ('Chicago', 'il', ['Cook County, IL']),\n",
    "    ('Houston', 'tx', ['Harris County, TX']),\n",
    "    ('Boston', 'ma', ['Suffolk County, MA', 'Middlesex County, MA']),\n",
    "    ('Phoenix', 'az', ['Maricopa County, AZ']),\n",
    "    ('Philadelphia', 'pa', ['Philadelphia County, PA']),\n",
    "    ('San Antonio', 'tx', ['Bexar County, TX']),\n",
    "    ('San Diego', 'ca', ['San Diego County, CA']),\n",
    "    ('Dallas', 'tx', ['Dallas County, TX']),\n",
    "    ('San Jose', 'ca', ['Santa Clara County, CA']),\n",
    "    ('Austin', 'tx', ['Travis County, TX']),\n",
    "]\n",
    "t_cities = [\n",
    "    ('New York City', 'ny', ['New York County, NY', 'Queens County, NY','Kings County, NY','Bronx County, NY','Richmond County, NY']),\n",
    "    ('Los Angeles', 'ca', ['Los Angeles County, CA']),\n",
    "    ('Chicago', 'il', ['Cook County, IL']),\n",
    "    ('Houston', 'tx', ['Harris County, TX']),\n",
    "    ('Boston', 'ma', ['Suffolk County, MA', 'Middlesex County, MA']),\n",
    "    ('Phoenix', 'az', ['Maricopa County, AZ']),\n",
    "    ('Philadelphia', 'pa', ['Philadelphia County, PA']),\n",
    "    ('San Antonio', 'tx', ['Bexar County, TX']),\n",
    "    ('San Diego', 'ca', ['San Diego County, CA']),\n",
    "    ('Dallas', 'tx', ['Dallas County, TX']),\n",
    "    ('San Jose', 'ca', ['Santa Clara County, CA']),\n",
    "    ('Austin', 'tx', ['Travis County, TX']),\n",
    "]\n",
    "\n",
    "\n",
    "fts_extractor_dim = 46\n",
    "edge_extractor_dim = 3\n",
    "GNNConfig2 = buildVNNConfig({'gat_num_heads':2,'transformer_num_heads':2,'attention_out_dim':8,\n",
    "                             'out_features': 16, 'layer_dims':[16]*2, 'initSeed': seed, \n",
    "                                     'actfuncFinal': torch.nn.Sigmoid(),'edge_dim':edge_extractor_dim})\n",
    "\n",
    "# early_stopper_train = EarlyStopper(patience=10, min_delta=1e-4)\n",
    "VNNConfig = buildVNNConfig({'layer_dims':[8], 'dropout': 0.33, \n",
    "                            'initSeed': seed, 'actfuncFinal': nn.ReLU()}) #nnSquare()\n",
    "splitByNodes = True\n",
    "externalities = True\n",
    "# if modelmode='g', conventional singly constrained gravity model, other parameters will be ignored\n",
    "modelmode = 'p'\n",
    "# gravi con, modelmode = 'p', attention = True, directEmebdding = False\n",
    "# GCN + MLP, modelmode = 'p', attention = False, VNNattraction = True\n",
    "attention = True\n",
    "VNNattraction = False\n",
    "directEmebdding = False #learn node embedding directly (MLP) or through GNNs\n",
    "filename = 'GravAttnLEHD-spatial-transfer-radiation-euclidean-GAT-resnet.csv'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "\n",
    "    # Pass this file object to csv.writer()\n",
    "    # and get a writer object\n",
    "    writer_object = writer(f)\n",
    "\n",
    "    # Pass the list as an argument into\n",
    "    # the writerow()\n",
    "    writer_object.writerow(['source-target']+[city for city,state, counties in cities])\n",
    "\n",
    "    # Close the file object\n",
    "    f.close()\n",
    "        \n",
    "for city,state, counties in cities:\n",
    "    splitSeed = 0\n",
    "    seed = 0\n",
    "\n",
    "    mainepochs = 500\n",
    "\n",
    "    lr = [1e-3]\n",
    "        \n",
    "\n",
    "    #some basic stats\n",
    "    print('loading datasets')\n",
    "    A_train = torch.load('training/'+city+'_A_train.pt').to(torch.device(\"cuda\"))\n",
    "    OD_train = torch.load('training/'+city+'_OD_train.pt')\n",
    "    between_fts_train = torch.load('training/'+city+'_between_fts_train.pt')\n",
    "    nodefts_train = torch.load('training/'+city+'_nodefts_train.pt')\n",
    "    \n",
    "\n",
    "    \n",
    "    RAE_list = []\n",
    "    CPC_list = []\n",
    "    MAE_list = []\n",
    "    pearson_list = []\n",
    "    jenson_list = []\n",
    "    \n",
    "    for target_city,target_state, target_counties in t_cities:\n",
    "\n",
    "        print('loading target LEHD:',target_city)\n",
    "        \n",
    "        A_valid = torch.load('training/'+target_city+'_A_valid.pt').to(torch.device(\"cuda\"))\n",
    "        OD_valid = torch.load('training/'+target_city+'_OD_valid.pt')\n",
    "        between_fts_valid = torch.load('training/'+target_city+'_between_fts_valid.pt')\n",
    "        nodefts_valid = torch.load('training/'+target_city+'_nodefts_valid.pt')\n",
    "    \n",
    "        target_A_test = torch.load('training/'+target_city+'_A_test.pt').to(torch.device(\"cuda\"))\n",
    "        target_OD_test = torch.load('training/'+target_city+'_OD_test.pt')\n",
    "        target_between_fts_test = torch.load('training/'+target_city+'_between_fts_test.pt')\n",
    "        target_nodefts_test = torch.load('training/'+target_city+'_nodefts_test.pt')\n",
    "        # standardization\n",
    "        \n",
    "        scaler = StandardScaler().fit(nodefts_train)\n",
    "        nodefts_train_t = torch.FloatTensor(scaler.transform(nodefts_train)).to(torch.device(\"cuda\"))\n",
    "        nodefts_valid_t = torch.FloatTensor(scaler.transform(nodefts_valid)).to(torch.device(\"cuda\"))\n",
    "        target_nodefts_test_t = torch.FloatTensor(scaler.transform(target_nodefts_test)).to(torch.device(\"cuda\"))\n",
    "        \n",
    "        bet_scaler = StandardScaler().fit(between_fts_train)\n",
    "        between_fts_train_t = torch.FloatTensor(bet_scaler.transform(between_fts_train)).to(torch.device(\"cuda\"))\n",
    "        between_fts_valid_t = torch.FloatTensor(bet_scaler.transform(between_fts_valid)).to(torch.device(\"cuda\"))\n",
    "        target_between_fts_test_t = torch.FloatTensor(bet_scaler.transform(target_between_fts_test)).to(torch.device(\"cuda\"))\n",
    "        \n",
    "#         feature extractor\n",
    "#         print('-----------feature extractor working-----------------')\n",
    "\n",
    "#         fts_trainer = AdversarialTrainer(input_dim=nodefts_train.shape[1], \n",
    "#                           feature_dim=fts_extractor_dim, learning_rate=1e-3)\n",
    "#         fts_trainer.cuda()\n",
    "#         source_data = nodefts_train_t\n",
    "#         target_data = target_nodefts_test_t\n",
    "#         valid_data = nodefts_valid_t\n",
    "#         source_labels = torch.ones(source_data.shape[0], 1).to(torch.device(\"cuda\"))\n",
    "#         target_labels = torch.zeros(target_data.shape[0], 1).to(torch.device(\"cuda\"))\n",
    "#         valid_labels = torch.ones(valid_data.shape[0], 1).to(torch.device(\"cuda\"))\n",
    "#         fts_trainer.train(source_data, target_data, source_labels, target_labels,target_data,\n",
    "#                           target_labels, num_epochs=1000)\n",
    "#         fts_trainer.feature_extractor.eval()\n",
    "#         nodefts_train_t = fts_trainer.feature_extractor(nodefts_train_t).detach()\n",
    "#         nodefts_valid_t = fts_trainer.feature_extractor(nodefts_valid_t).detach()\n",
    "#         target_nodefts_test_t = fts_trainer.feature_extractor(target_nodefts_test_t).detach()\n",
    "\n",
    "\n",
    "\n",
    "#         edge_fts_trainer = AdversarialTrainer(input_dim=between_fts_train.shape[1], \n",
    "#                                  feature_dim=edge_extractor_dim, learning_rate=1e-3)\n",
    "#         edge_fts_trainer.cuda()\n",
    "#         source_data = between_fts_train_t\n",
    "#         target_data = target_between_fts_test_t\n",
    "#         valid_data = between_fts_valid_t\n",
    "#         source_labels = torch.ones(source_data.shape[0], 1).to(torch.device(\"cuda\"))\n",
    "#         target_labels = torch.zeros(target_data.shape[0], 1).to(torch.device(\"cuda\"))\n",
    "#         valid_labels = torch.zeros(valid_data.shape[0], 1).to(torch.device(\"cuda\"))\n",
    "#         edge_fts_trainer.train(source_data, target_data, source_labels, target_labels,valid_data,\n",
    "#                                valid_labels,num_epochs=1000)\n",
    "#         edge_fts_trainer.feature_extractor.eval()\n",
    "#         between_fts_train_t = edge_fts_trainer.feature_extractor(between_fts_train_t).detach()\n",
    "#         between_fts_valid_t = edge_fts_trainer.feature_extractor(between_fts_valid_t).detach()\n",
    "#         target_between_fts_test_t = edge_fts_trainer.feature_extractor(target_between_fts_test_t).detach()\n",
    "        \n",
    "        NRL = GMLearning(city,nodefts_train_t,GNNConfig2, \n",
    "                         VNNConfig,modelmode,directembedding = directEmebdding,\n",
    "                         attention=attention,VNNattraction=VNNattraction) \n",
    "        NRL = NRL.cuda()\n",
    "\n",
    "        NRL.fit(city,A_train,nodefts_train_t,OD_train,between_fts_train_t,A_valid,\n",
    "        nodefts_valid_t,OD_valid,between_fts_valid_t,n_epochs = mainepochs,lr = lr, \n",
    "        interim_output_freq = mainepochs//10)\n",
    "\n",
    "        with torch.no_grad(): \n",
    "\n",
    "            target_y_test = NRL.forward(target_A_test,target_nodefts_test_t,\n",
    "                                        target_OD_test,target_between_fts_test_t)\n",
    "            target_y_test = torch.nan_to_num(target_y_test)\n",
    "        # test nodes performance, edges between train and test nodes\n",
    "\n",
    "\n",
    "        rae = round(NRL.loss(target_y_test.cpu(),torch.FloatTensor(target_OD_test)).item(),3)\n",
    "        print('KL', rae)\n",
    "        \n",
    "        cpc = common_part_of_commuters(torch.FloatTensor(target_OD_test),target_y_test.cpu())\n",
    "        if type(cpc) == float:\n",
    "            pass\n",
    "        else:\n",
    "            cpc = cpc.item()\n",
    "        cpc = round(cpc,3)\n",
    "        print('CPC', cpc)\n",
    "        mae = round(MAE(torch.FloatTensor(target_OD_test),target_y_test.cpu()).item(),3)\n",
    "        print('MAE',mae)\n",
    "        pearson = round(scipy.stats.pearsonr(target_OD_test.flatten(),\n",
    "                                             target_y_test.cpu().detach().numpy().flatten())[0],3)\n",
    "        print('pearson', pearson)\n",
    "        jenson = round(scipy.spatial.distance.jensenshannon(target_OD_test.flatten(),\n",
    "                                                            target_y_test.cpu().detach().numpy().flatten()),3)\n",
    "        print('jenson',jenson)\n",
    "\n",
    "        RAE_list += [rae]\n",
    "        CPC_list += [cpc]\n",
    "        MAE_list += [mae]\n",
    "        pearson_list += [pearson]\n",
    "        jenson_list += [jenson]\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    with open(filename, 'a') as f:\n",
    "\n",
    "\n",
    "        writer_object = writer(f)\n",
    "\n",
    "\n",
    "        writer_object.writerow([city+' KL']+RAE_list)\n",
    "        \n",
    "        writer_object.writerow([city+' CPC']+CPC_list)\n",
    "        writer_object.writerow([city+' MAE']+MAE_list)\n",
    "        writer_object.writerow([city+ 'Pea']+pearson_list)\n",
    "        writer_object.writerow([city+ 'Jen']+jenson_list)\n",
    "\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de363e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
